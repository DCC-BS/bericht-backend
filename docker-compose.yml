services:
  vllm_qwen3_32B:
    platform: linux/amd64
    image: vllm/vllm-openai:v0.8.5
    container_name: vllm_qwen3_32B
    ports:
      - ${LLM_API_PORT}:8000
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_AUTH_TOKEN}
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    ipc: host
    volumes:
      - ${HUGGING_FACE_CACHE_DIR}:/root/.cache/huggingface
    command: >
      --port 8000 --model Qwen/Qwen3-32B-AWQ --max-model-len 40960 --gpu-memory-utilization 0.75 --enable-auto-tool-choice --tool-call-parser hermes --tensor-parallel-size 2 --enable-reasoning --reasoning-parser deepseek_r1

  faster_whisper:
    platform: linux/amd64
    build:
      context: https://github.com/DCC-BS/bentoml-faster-whisper.git
      dockerfile: ./Dockerfile
    ports:
      - '50001:50001'
    environment:
      - http_proxy
      - HTTP_PROXY
      - https_proxy
      - HTTPS_PROXY
      - no_proxy
      - NO_PROXY
      - TIMEOUT
      - MAX_CONCURRENCY
      - MAX_BATCH_SIZE
      - MAX_LATENCY_MS
      - HF_AUTH_TOKEN=${HF_AUTH_TOKEN}
      - UV_HTTP_TIMEOUT=300
    volumes:
      - hugging_face_cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          memory: 16g
          devices:
            - driver: nvidia
              device_ids: [ '1' ]
              capabilities: [ gpu ]
volumes:
  hugging_face_cache:
