services:
  vllm_gemma3_27b:
    platform: linux/amd64
    build:
      context: .
      dockerfile: ./Dockerfile.LLM
    # image: vllm/vllm-openai:v0.8.5
    container_name: vllm_gemma3_27b
    ports:
      - 50002:8000
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    ipc: host
    volumes:
      - ${HUGGING_FACE_CACHE_DIR}:/root/.cache/huggingface
    command: >
      --port 8000
      --model ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g
      --max-model-len 32000
      --gpu-memory-utilization 0.83
      --max-num-seqs 1
      --tensor-parallel-size 2
      --dtype bfloat16 
      --block-size 32

  faster_whisper:
    platform: linux/amd64
    build:
      context: https://github.com/DCC-BS/bentoml-faster-whisper.git
      dockerfile: ./Dockerfile
    ports:
      - '50001:50001'
    environment:
      - http_proxy
      - HTTP_PROXY
      - https_proxy
      - HTTPS_PROXY
      - no_proxy
      - NO_PROXY
      - TIMEOUT
      - MAX_CONCURRENCY
      - MAX_BATCH_SIZE
      - MAX_LATENCY_MS
      - HF_AUTH_TOKEN=${HF_AUTH_TOKEN}
      - UV_HTTP_TIMEOUT=300
    volumes:
      - hugging_face_cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          memory: 16g
          devices:
            - driver: nvidia
              device_ids: [ '1' ]
              capabilities: [ gpu ]
  bericht-backend:
    build:
      context: .
      dockerfile: ./Dockerfile
    ports:
      - "8000:8000"
    environment:
      - WHISPER_API=http://faster_whisper:50001/v1
volumes:
  hugging_face_cache:
